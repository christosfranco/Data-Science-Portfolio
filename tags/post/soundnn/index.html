<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Urban Sound Processing and Neural Network Model | Main Page</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Analyzing sounds that are common in an urban enviroment and training a model for recognizing different labeled sounds.">
    <meta name="generator" content="Hugo 0.80.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    

  
  
    <link rel="stylesheet" href="https://christosfranco.github.io/Data-Science-Portfolio/ananke/dist/main.css_5c99d70a7725bacd4c701e995b969fea.css" >
  




    
      

    

    
    
    <meta property="og:title" content="Urban Sound Processing and Neural Network Model" />
<meta property="og:description" content="Analyzing sounds that are common in an urban enviroment and training a model for recognizing different labeled sounds." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://christosfranco.github.io/Data-Science-Portfolio/post/soundnn/" />
<meta property="article:published_time" content="2021-02-12T11:25:05-04:00" />
<meta property="article:modified_time" content="2021-02-12T11:25:05-04:00" /><meta property="og:site_name" content="Main Page" />
<meta itemprop="name" content="Urban Sound Processing and Neural Network Model">
<meta itemprop="description" content="Analyzing sounds that are common in an urban enviroment and training a model for recognizing different labeled sounds.">
<meta itemprop="datePublished" content="2021-02-12T11:25:05-04:00" />
<meta itemprop="dateModified" content="2021-02-12T11:25:05-04:00" />
<meta itemprop="wordCount" content="1008">



<meta itemprop="keywords" content="neural network,machine learning,sound," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Urban Sound Processing and Neural Network Model"/>
<meta name="twitter:description" content="Analyzing sounds that are common in an urban enviroment and training a model for recognizing different labeled sounds."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://christosfranco.github.io/Data-Science-Portfolio/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Main Page
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://christosfranco.github.io/Data-Science-Portfolio/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://christosfranco.github.io/Data-Science-Portfolio/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://christosfranco.github.io/Data-Science-Portfolio/post/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      







<a href="https://linkedin.com/in/christian-arboe-franck/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/christosfranco/" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://christosfranco.github.io/Data-Science-Portfolio/post/soundnn/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://christosfranco.github.io/Data-Science-Portfolio/post/soundnn/&amp;text=Urban%20Sound%20Processing%20and%20Neural%20Network%20Model" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://christosfranco.github.io/Data-Science-Portfolio/post/soundnn/&amp;title=Urban%20Sound%20Processing%20and%20Neural%20Network%20Model" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Urban Sound Processing and Neural Network Model</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2021-02-12T11:25:05-04:00">February 12, 2021</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p><a href="https://github.com/christosfranco/Signal-Processing-and-Voice-Recognition">Link to github repo</a></p>
<p><a href="https://www.kaggle.com/chrfkaggle/urban-sounds">Link to kaggle repo</a></p>
<p><a href="https://urbansounddataset.weebly.com/urbansound8k.html#10foldCV">Link To Dataset</a></p>
<h1 id="data-exploration">Data exploration</h1>
<p>The dataset we want to analyze consists of different soundbites each of length 1 second containing the highest intensity in a larger soundbite. The dataset has sounds such as a dog barking, children playing, car horn, and streetmusic. Lets have a look and see how the different kinds of sounds look like in comparison to one another.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<figure>
    <img src="https://christosfranco.github.io/Data-Science-Portfolio/images/soundUrban/soundbitess.png"/> 
</figure>

<h1 id="feature-extraction">Feature extraction</h1>
<p>The features we will use to analyze the difference in these soundbites will be the Mel-frequency cepstral coefficients (MFCCs). We will use 40 coefficients to describe our sound bite. The librosa library will be used to apply the functions nessesary to get to this representation of the features.</p>
<p>The process to arrive at the MFCCs are.</p>
<p>Taking the Fourier transform</p>
<p>Map the powers of the spectrum to the mel scale</p>
<p>Take the log of the powers</p>
<p>Take the discrete cos transform</p>
<p>Out we will get the amplitudes of the spectrum</p>
<p>The MFCC are useful for sound classification as they resemble the human spectrum. In short it has a higher destrinction between sounds of powered amplitudes and frequencies. Humans usually experience sounds in a radical way, as to say that a linear increase in sound amplitude will by a human be interpreted of a much higher degree. The MFCCs contains the information for the soundbites, we will use these later.</p>
<h1 id="preparing-data-for-training">Preparing data for training</h1>
<p>For easier processing of the data we will shift and normalize the data. Shifting the data close to 0 and normalizing them approximately close from -1 to 1, makes computations faster.</p>
<p>The labels, or the predetermined category that the sound lies in, will be encoded into a vector of length number of different sounds. The model will then try to determine which of these categories the sound lies in with a probablity. The softmax activation layer will then return the highest probability as the label that is most probable to be true.</p>
<h1 id="neural-network-model">Neural Network model</h1>
<p>We will train the data on a neural network model with dense layers and RELU activation layers. The input shape will be all of our train datapoints with each 40 features. The output will be of the shape of the number of labels that we have, 10.</p>
<p>For training the model we will use a validation set, that evaluates the weights of the model after each epoch. The weights are updated according to how they best influence the validation datapoints in a direction that gives a higher accuracy.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">num_labels <span style="color:#f92672">=</span> yy<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_model_graph</span>(input_shape<span style="color:#f92672">=</span>(n_mfcc,)):
    model <span style="color:#f92672">=</span> Sequential()
    model<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">256</span>))
    model<span style="color:#f92672">.</span>add(Activation(<span style="color:#e6db74">&#39;relu&#39;</span>))
    model<span style="color:#f92672">.</span>add(Dropout(<span style="color:#ae81ff">0.5</span>))
    model<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">256</span>))
    model<span style="color:#f92672">.</span>add(Activation(<span style="color:#e6db74">&#39;relu&#39;</span>))
    model<span style="color:#f92672">.</span>add(Dropout(<span style="color:#ae81ff">0.5</span>))
    model<span style="color:#f92672">.</span>add(Dense(num_labels))
    model<span style="color:#f92672">.</span>add(Activation(<span style="color:#e6db74">&#39;softmax&#39;</span>))
    <span style="color:#75715e"># Compile the model</span>
    model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>], optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>)    
    <span style="color:#66d9ef">return</span> model
model <span style="color:#f92672">=</span> build_model_graph()</code></pre></div>
<h1 id="evaluating-the-model-process">Evaluating the model process</h1>
<p>After training the model with a batchsize of 256 (the number of datapoints that are each clumbed together with each epoch at random) on 1000 epochs, we have the following development of our loss and accuracy on the validation and the training set.</p>
<figure>
    <img src="https://christosfranco.github.io/Data-Science-Portfolio/images/soundUrban/acclossplot.png"/> 
</figure>

<p>We can see that the accuracy of the training set is about 10 percentage points higher that the validation accuracy, the loss is falling for the training set, but it is starting to increase for the validation set. The model can still be trained, as we have not seen a convergence in the accuracy yet. A danger will be that we overfit the model to the training set, and thus can expect lower accuracy on test data.</p>
<p>Lets try to see how high our testing set fares with the model that we have created.
<figure>
    <img src="https://christosfranco.github.io/Data-Science-Portfolio/images/soundUrban/acc.png"/> 
</figure>
</p>
<p>As we saw in the plot above the training accuracy is at ~91%, while the testing accuracy is ~84%.</p>
<h1 id="looking-at-the-wrongly-predicted-sound-bites">Looking at the wrongly predicted sound bites</h1>
<!-- raw HTML omitted -->
<p>Below we have a table of the predicted sounds from the model, and the predetermined labels given to the sound bites. We can see that children_playing is predicted far too often for the model. It classifies this category mainly for sounds that are dog_bark, gun_shot, and street_music. Two of them might have some correlation, but the fact that the model classifies a gun_shot as children_playing can have obvious implications if this model were to be released into a real world application.<br>
<figure>
    <img src="https://christosfranco.github.io/Data-Science-Portfolio/images/soundUrban/combined.png"/> 
</figure>
</p>
<p>We can play the sounds to hear for ourselves, and try to find an explanation for why the model predicted as it did.</p>
<p>To have a closer look at all the sound bites that the model predicted wrong have a look at the kaggle.</p>
<p>We use the following code to run through all the sound bites.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">    <span style="color:#75715e">#Play a sound from the dataset</span>
    sound_file <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;/kaggle/input/urbansounds/UrbanSound8K/audio/&#39;</span>
    <span style="color:#66d9ef">for</span> index, row <span style="color:#f92672">in</span> df_unequal<span style="color:#f92672">.</span>iterrows():
        slice_name <span style="color:#f92672">=</span> row[<span style="color:#e6db74">&#39;file_name&#39;</span>]
        
        name<span style="color:#f92672">=</span> sound_file<span style="color:#f92672">+</span>slice_name
        
        print(<span style="color:#e6db74">&#34;Name of file: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> , Predicted to be: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> , Real Value: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> &#34;</span><span style="color:#f92672">.</span>format(slice_name , row[<span style="color:#e6db74">&#39;preds&#39;</span>] ,row[<span style="color:#e6db74">&#39;class_labels&#39;</span>]))
        
        input(ipd<span style="color:#f92672">.</span>display(ipd<span style="color:#f92672">.</span>Audio(name,autoplay<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)))
        clear_output(wait<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)</code></pre></div>
<p>The below sound bite is an example from which I think the model actually does a better job than who classified the sound bite in the dataset.</p>
<p>One instance in particular, where the sound is prelabeled to be a car_horn, and predicted to be a jackhammer. But the car_horn is in the background, and the jackhammer is in the foreground. So I think the classifier actually does a better job here than the human person who classified the sound bite.</p>
<p>Audio('/kaggle/input/urbansounds/UrbanSound8K/audio/fold4/7389-1-0-4.wav')</p>
<p>The sound bite below is an example of a sound bite that doesnt really classifies as any of the sound labels. The soundbite is just noise, and this should probably be emitted from the dataset.</p>
<p>Audio('/kaggle/input/urbansounds/UrbanSound8K/audio/fold4/194458-9-1-75.wav')</p>
<p>Some sound bites are so low in amplitude that is not possible to distinguish what the sound actually is. These sound bites should also be emitted from the dataset.</p>
<h1 id="in-conclusion">In conclusion</h1>
<p>The model actually does a fairly well job, and eventhough it not able to classify all sound right, there could be made a case that these sound bites are either of so low amplitude that the sound cannot be heard or that the label is many fold (that the sound bite can actually be labeled as multiple of the possible categories).</p>
<p><a href="https://github.com/christosfranco/Data-Processing-and-SQL-Analytics">Link to github repo</a></p>
<p><a href="https://www.kaggle.com/chrfkaggle/urban-sounds">Link to kaggle repo</a></p>
<p><a href="https://urbansounddataset.weebly.com/urbansound8k.html#10foldCV">Link To Dataset</a></p>
<p><a href="https://christosfranco.github.io/Data-Science-Portfolio/post/soundnn/">Go to top</a></p>
<ul class="pa0">
  
   <li class="list">
     <a href="https://christosfranco.github.io/Data-Science-Portfolio/tags/neural-network" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">neural network</a>
   </li>
  
   <li class="list">
     <a href="https://christosfranco.github.io/Data-Science-Portfolio/tags/machine-learning" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">machine learning</a>
   </li>
  
   <li class="list">
     <a href="https://christosfranco.github.io/Data-Science-Portfolio/tags/sound" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">sound</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://christosfranco.github.io/Data-Science-Portfolio/" >
    &copy;  Main Page 2021 
  </a>
    <div>







<a href="https://linkedin.com/in/christian-arboe-franck/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/christosfranco/" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







</div>
  </div>
</footer>

  </body>
</html>
